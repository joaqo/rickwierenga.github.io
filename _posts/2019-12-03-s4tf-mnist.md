---
layout: post
title: Your first Swift for TensorFlow model
category: s4tf
---

Swift for TensorFlow is TensorFlow, implemented in Swift. Its Python counterpart ("TensorFlow") is one of the best deep learning libraries ever made, but it's hitting its limits. Python, being interpreted, is quite a slow language. Python is also limiting in many other ways, it lacks features that could make your life, as a programmer, much easier. 

So the TensorFlow team started looking for a new, better language. They chose Swift: a new, open source programming language originally developed by Apple, with a great focus on speed and readability. So here we are, Swift for TensorFlow is in the making. Even though the library lacks many features at this point, it is useable already. For some projects, that is. In this post I would like to show you one of those projects, and give you an introduction to the framework.

If you do not already know Swift, you will be surprised by the sheer amount of high quality learning resources available. The best place to start is [the swift website](https://www.swift.org). On the website, you will find [a complete book to learn the language](https://docs.swift.org/swift-book/index.html).

The Swift for TensorFlow (S4TF) project is an extension of the Swift programming language with many additional features, beyond just deep learning. [Here](https://www.tensorflow.org/swift/) is its homepage.

Getting started with S4TF can be done without any setup using Google Colab. Swift is not currently registered as a default kernel, so you should use [this](https://colab.research.google.com/github/tensorflow/swift/blob/master/notebooks/blank_swift.ipynb) empty notebook as a starting point when developing Swift for TensorFlow applications. 

Let's see how to solve MNIST using Swift.

## Downloading dependencies
Google Colab currently uses a slightly outdated version of S4TF (0.5 vs 0.6) which means we can't use the fancy [swift-models](https://github.com/tensorflow/swift-models/tree/master/Datasets) library to load datasets. Luckily we can work around it by recreating the MNIST dataset implementation.

The following code is copied from the swift-models repo. Don't worry too much about the specifics for now, though it is interesting to read Swift code written by Google engineers.
```swift
public struct LabeledExample: TensorGroup {
    public var label: Tensor<Int32>
    public var data: Tensor<Float>

    public init(label: Tensor<Int32>, data: Tensor<Float>) {
        self.label = label
        self.data = data
    }

    public init<C: RandomAccessCollection>(
        _handles: C
    ) where C.Element: _AnyTensorHandle {
        precondition(_handles.count == 2)
        let labelIndex = _handles.startIndex
        let dataIndex = _handles.index(labelIndex, offsetBy: 1)
        label = Tensor<Int32>(handle: TensorHandle<Int32>(handle: _handles[labelIndex]))
        data = Tensor<Float>(handle: TensorHandle<Float>(handle: _handles[dataIndex]))
    }
}

public struct DatasetUtilities {
    public static let currentWorkingDirectoryURL = URL(
        fileURLWithPath: FileManager.default.currentDirectoryPath)

    public static func fetchResource(
        filename: String,
        remoteRoot: URL,
        localStorageDirectory: URL = currentWorkingDirectoryURL
    ) -> Data {
        print("Loading resource: \(filename)")

        let resource = ResourceDefinition(
            filename: filename,
            remoteRoot: remoteRoot,
            localStorageDirectory: localStorageDirectory)

        let localURL = resource.localURL

        if !FileManager.default.fileExists(atPath: localURL.path) {
            print(
                "File does not exist locally at expected path: \(localURL.path) and must be fetched"
            )
            fetchFromRemoteAndSave(resource)
        }

        do {
            print("Loading local data at: \(localURL.path)")
            let data = try Data(contentsOf: localURL)
            print("Succesfully loaded resource: \(filename)")
            return data
        } catch {
            fatalError("Failed to contents of resource: \(localURL)")
        }
    }

    struct ResourceDefinition {
        let filename: String
        let remoteRoot: URL
        let localStorageDirectory: URL

        var localURL: URL {
            localStorageDirectory.appendingPathComponent(filename)
        }

        var remoteURL: URL {
            remoteRoot.appendingPathComponent(filename).appendingPathExtension("gz")
        }

        var archiveURL: URL {
            localURL.appendingPathExtension("gz")
        }
    }

    static func fetchFromRemoteAndSave(_ resource: ResourceDefinition) {
        let remoteLocation = resource.remoteURL
        let archiveLocation = resource.archiveURL

        do {
            print("Fetching URL: \(remoteLocation)...")
            let archiveData = try Data(contentsOf: remoteLocation)
            print("Writing fetched archive to: \(archiveLocation.path)")
            try archiveData.write(to: archiveLocation)
        } catch {
            fatalError("Failed to fetch and save resource with error: \(error)")
        }
        print("Archive saved to: \(archiveLocation.path)")

        extractArchive(for: resource)
    }

    static func extractArchive(for resource: ResourceDefinition) {
        print("Extracting archive...")

        let archivePath = resource.archiveURL.path

        #if os(macOS)
            let gunzipLocation = "/usr/bin/gunzip"
        #else
            let gunzipLocation = "/bin/gunzip"
        #endif

        let task = Process()
        task.executableURL = URL(fileURLWithPath: gunzipLocation)
        task.arguments = [archivePath]
        do {
            try task.run()
            task.waitUntilExit()
        } catch {
            fatalError("Failed to extract \(archivePath) with error: \(error)")
        }
    }
}

public struct MNIST {
    public let trainingDataset: Dataset<LabeledExample>
    public let testDataset: Dataset<LabeledExample>
    public let trainingExampleCount = 60000

    public init() {
        self.init(flattening: false, normalizing: false)
    }

    public init(
        flattening: Bool = false, normalizing: Bool = false,
        localStorageDirectory: URL = DatasetUtilities.currentWorkingDirectoryURL
    ) {
        self.trainingDataset = Dataset<LabeledExample>(
            elements: fetchDataset(
                localStorageDirectory: localStorageDirectory,
                imagesFilename: "train-images-idx3-ubyte",
                labelsFilename: "train-labels-idx1-ubyte",
                flattening: flattening,
                normalizing: normalizing))

        self.testDataset = Dataset<LabeledExample>(
            elements: fetchDataset(
                localStorageDirectory: localStorageDirectory,
                imagesFilename: "t10k-images-idx3-ubyte",
                labelsFilename: "t10k-labels-idx1-ubyte",
                flattening: flattening,
                normalizing: normalizing))
    }
}

fileprivate func fetchDataset(
    localStorageDirectory: URL,
    imagesFilename: String,
    labelsFilename: String,
    flattening: Bool,
    normalizing: Bool
) -> LabeledExample {
    guard let remoteRoot:URL = URL(string: "http://yann.lecun.com/exdb/mnist") else {
        fatalError("Failed to create MNST root url: http://yann.lecun.com/exdb/mnist")
    }

    let imagesData = DatasetUtilities.fetchResource(
        filename: imagesFilename,
        remoteRoot: remoteRoot,
        localStorageDirectory: localStorageDirectory)
    let labelsData = DatasetUtilities.fetchResource(
        filename: labelsFilename,
        remoteRoot: remoteRoot,
        localStorageDirectory: localStorageDirectory)

    let images = [UInt8](imagesData).dropFirst(16).map(Float.init)
    let labels = [UInt8](labelsData).dropFirst(8).map(Int32.init)

    let rowCount = labels.count
    let (imageWidth, imageHeight) = (28, 28)

    if flattening {
        var flattenedImages = Tensor(shape: [rowCount, imageHeight * imageWidth], scalars: images)
            / 255.0
        if normalizing {
            flattenedImages = flattenedImages * 2.0 - 1.0
        }
        return LabeledExample(label: Tensor(labels), data: flattenedImages)
    } else {
        return LabeledExample(
            label: Tensor(labels),
            data:
                Tensor(shape: [rowCount, 1, imageHeight, imageWidth], scalars: images)
                .transposed(withPermutations: [0, 2, 3, 1]) / 255  // NHWC
        )
    }
}
```

If Google Colab gets updated, you can use just the following:

```swift
%install-location $cwd/swift-install
%install '.package(url: "https://github.com/tensorflow/swift-models", .branch("tensorflow-0.6"))' Datasets
```

Brad Larson from the core team has [said](https://github.com/tensorflow/swift-models/issues/233) they are working on an update.

Now we can initialize an MNIST instance.

```swift
let mnist = MNIST(flattening: true, normalizing: true)
```

Before we can go ahead and implement the neural network, we must specify two more convenience constants:

```swift
let imageHeight = 28, imageWidth = 28
let imageSize = imageHeight * imageWidth
```

## Building the network
Our network will have just one hidden layer with 300 hidden nodes.

This is how you would define a model in Swift:
```swift
struct Model: Layer {
    var hiddenLayer = Dense<Float>(inputSize: imageSize, outputSize: 300, activation: relu)
    var outputLayer = Dense<Float>(inputSize: 300, outputSize: 10, activation: softmax)

    @differentiable
    func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {
        return input.sequenced(through: hiddenLayer, outputLayer)
    }
}
```

The `callAsFunction` function will be called when we feed forward `input` through the neural network.

Now create an instance of `Model`:
```swift
var model = Model()
```

This needs to be a variable because it will be changed througout training.

## Training the network
When training a neural network you should always be using batches. This not only speeds up training significantly, but it also improves accuracy. We will use a batch size of 512 here. We also set the test batches before we start training so we only have to do it once. Because we will use shuffled batches during training, we do not yet set the training batches.

```swift
let batchSize = 512
let testBatches = mnist.testDataset.batched(batchSize)
```

Next we need an optimizer. Optimizers are, like many things in S4TF, also structs. So we need to initialize one:

```swift
let optimizer = Adam(for: model)
```

Now we are ready to implement a training loop:

```swift
for epoch in 1...10 {
  ...
}
```

In this loop we want to do two things: 

1. Update the parameters
2. Evaluate the model and print out the results

### 1. Updating the parameters
When you are training a model in TensorFlow you should tell it. TensorFlow will automatically adapt to your current learning phase. For example, dropout will be applied during learning, but not during inference.

```swift
Context.local.learningPhase = .training
```

Next we grab a set of training batches--shuffled of course:

```swift
let trainingShuffled = mnist.trainingDataset.shuffled(sampleCount: mnist.trainingExampleCount, randomSeed: Int64(epoch))
```

Then we loop over the batches, calculating the gradient at each step. We can apply gradients by passing them, and a pointer to the model, to our optimizer.

```swift
for batch in trainingShuffled.batched(batchSize) {
    let (labels, images) = (batch.label, batch.data)
    let (_, gradients) = valueWithGradient(at: model) { model -> Tensor<Float> in
        let logits = model(images)
        return softmaxCrossEntropy(logits: logits, labels: labels)
    }
    optimizer.update(&model, along: gradients)
}
```

That's it. Our model will learn now.

### 2. Evaluating the model
Sitting there and waiting for a model to finish training is boring. Sitting there and looking at the accuracy improving over time is fascinating though, so let's make that happen.

We first set the learning phase to inference. Then we loop over the training set and test set and accumulate the correct predictions. Finally, we print out the results.

```swift
Context.local.learningPhase = .inference

var correctTrainGuessCount = 0
var totalTrainGuessCount = 0
for batch in mnist.trainingDataset.batched(512) {
    let (labels, images) = (batch.label, batch.data)
    let logits = model(images)
    let correctPredictions = logits.argmax(squeezingAxis: 1) .== labels
    correctTrainGuessCount += Int(Tensor<Int32>(correctPredictions).sum().scalarized())
    totalTrainGuessCount += batchSize
}
let trainAcc = Float(correctTrainGuessCount) / Float(totalTrainGuessCount)

var correctValGuessCount = 0
var totalValGuessCount = 0
for batch in testBatches {
    let (labels, images) = (batch.label, batch.data)
    let logits = model(images)
    let correctPredictions = logits.argmax(squeezingAxis: 1) .== labels
    correctValGuessCount += Int(Tensor<Int32>(correctPredictions).sum().scalarized())
    totalValGuessCount += batchSize
}
let valAcc = Float(correctValGuessCount) / Float(totalValGuessCount)

print("\(epoch) | Training accuracy: \(trainAcc) | Validation accuracy: \(valAcc)")
```

You can now run the training loop and see the model learn!

## Conclusion
As you've seen it's not too hard to build and train a neural network in Swift, but it's definitely harder than Python. In this case, that is. Swift has many more things to offer, and I'm sure we will see some great improvements over the next few months.

